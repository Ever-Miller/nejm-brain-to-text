import pandas as pd
import torch
import numpy as np
from datasets import Dataset
import evaluate
import os 
import glob

# LoRA/PEFT åº“å¯¼å…¥
from peft import LoraConfig, get_peft_model, TaskType
from transformers import PreTrainedTokenizer 
from transformers import (
    AutoTokenizer, 
    BertConfig, 
    BertModel,
    BertForMaskedLM, 
    GPT2Config, 
    GPT2LMHeadModel, 
    EncoderDecoderConfig, 
    EncoderDecoderModel,
    Seq2SeqTrainingArguments, 
    Seq2SeqTrainer,
    TrainingArguments, 
    Trainer, 
    DataCollatorForSeq2Seq,
    DataCollatorForLanguageModeling, 
    EarlyStoppingCallback
)

# ==========================================
# 1. å…¨å±€é…ç½® & éŸ³ç´ å®šä¹‰
# ==========================================

PHONEMES_LIST = [
    "AA", "AE", "AH", "AO", "AW", "AY", "B", "CH", "D", "DH",
    "EH", "ER", "EY", "F", "G", "HH", "IH", "IY", "JH", "K",
    "L", "M", "N", "NG", "OW", "OY", "P", "R", "S", "SH",
    "T", "TH", "UH", "UW", "V", "W", "Y", "Z", "ZH", "BLANK", 
    "[MASK]" 
]

SPECIAL_TOKENS = ['<pad>', '<sos>', '<eos>', '.', '|'] 
PHONEME_VOCAB = SPECIAL_TOKENS + PHONEMES_LIST
phoneme_to_id = {p: i for i, p in enumerate(PHONEME_VOCAB)}
id_to_phoneme = {i: p for p, i in phoneme_to_id.items()}

VOCAB_SIZE_PHONEME = len(PHONEME_VOCAB)
PAD_ID_PHONEME = phoneme_to_id['<pad>']
SOS_ID = phoneme_to_id['<sos>']
EOS_ID = phoneme_to_id['<eos>']

try:
    MASK_ID_PHONEME = phoneme_to_id['[MASK]']
except KeyError:
    MASK_ID_PHONEME = PAD_ID_PHONEME 

# æ¨¡å‹ä¸è®­ç»ƒè¶…å‚æ•°
PLM_MODEL_NAME = "gpt2"      
MAX_P_LEN = 128              
MAX_S_LEN = 50               
ENCODER_HIDDEN_SIZE = 256    # å·²ä¼˜åŒ–ï¼šå‡å° Hidden Size
NUM_EPOCHS = 50              

# MPM é¢„è®­ç»ƒé…ç½®
MPM_PRETRAIN_EPOCHS = 10     
MPM_OUTPUT_DIR = "./mpm_pretrain_checkpoints"

# ==========================================
# 2. æ•°æ®å¤„ç†å‡½æ•°
# ==========================================

tokenizer = AutoTokenizer.from_pretrained(PLM_MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

# --- DummyTokenizer (ä¿®å¤ç‰ˆ) ---
class DummyPhonemeTokenizer:
    def __init__(self, pad_id, mask_id, sos_id, eos_id, vocab_size):
        self.pad_token_id = pad_id
        self.mask_token_id = mask_id
        self.sos_id = sos_id
        self.eos_id = eos_id
        self.vocab_size = vocab_size
        self.mask_token = "[MASK]"
        self.pad_token = "<pad>"
        self.all_special_ids = [pad_id, mask_id, sos_id, eos_id]

    def __len__(self):
        return self.vocab_size

    def get_vocab(self):
        return {"[MASK]": self.mask_token_id, "<pad>": self.pad_token_id}

    def save_pretrained(self, save_directory, **kwargs):
        pass

    def pad(self, encoded_inputs, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors=None):
        import torch
        input_ids = [example['input_ids'] for example in encoded_inputs]
        if return_tensors == 'pt':
            batch = {'input_ids': torch.tensor(input_ids, dtype=torch.long)}
            if len(encoded_inputs) > 0 and 'attention_mask' in encoded_inputs[0]:
                attention_masks = [example['attention_mask'] for example in encoded_inputs]
                batch['attention_mask'] = torch.tensor(attention_masks, dtype=torch.long)
            return batch
        return encoded_inputs

    def get_special_tokens_mask(self, token_ids_0, already_has_special_tokens=False):
        return [1 if token in self.all_special_ids else 0 for token in token_ids_0]

    def convert_tokens_to_ids(self, token):
        if token == self.mask_token: return self.mask_token_id
        if token == self.pad_token: return self.pad_token_id
        return 0 

# --- Collator (ä¿®å¤ç‰ˆ) ---
class PhonemeMaskingDataCollator(DataCollatorForLanguageModeling):
    def __init__(self, *args, **kwargs):
        dummy_tokenizer = DummyPhonemeTokenizer(
            pad_id=PAD_ID_PHONEME, 
            mask_id=MASK_ID_PHONEME, 
            sos_id=SOS_ID, 
            eos_id=EOS_ID, 
            vocab_size=VOCAB_SIZE_PHONEME
        )
        super().__init__(dummy_tokenizer, mlm=True, mlm_probability=0.15, **kwargs)
        self.pad_token_id = PAD_ID_PHONEME

# --- é¢„å¤„ç†å‡½æ•° ---
def preprocess_function(example):
    phoneme_seq = str(example['phonemes']).split()
    cleaned_phoneme_seq = [p for p in phoneme_seq if p not in ['BLANK', '[MASK]']]
    
    p_ids = [SOS_ID] + \
            [phoneme_to_id.get(p, PAD_ID_PHONEME) for p in cleaned_phoneme_seq] + \
            [EOS_ID]
    
    if len(p_ids) > MAX_P_LEN:
        p_ids = p_ids[:MAX_P_LEN]
        p_ids[-1] = EOS_ID 
        
    attention_mask_p = [1] * len(p_ids)
    
    padding_len = MAX_P_LEN - len(p_ids)
    if padding_len > 0:
        p_ids.extend([PAD_ID_PHONEME] * padding_len)
        attention_mask_p.extend([0] * padding_len)

    s_tokenized = tokenizer(
        example['sentence'], 
        max_length=MAX_S_LEN, 
        padding="max_length", 
        truncation=True
    )
    
    labels = s_tokenized['input_ids'].copy()
    labels = [l if l != tokenizer.pad_token_id else -100 for l in labels]

    return {
        'input_ids': p_ids,          
        'attention_mask': attention_mask_p, 
        'labels': labels,            
    }

def mpm_preprocess_function(example):
    # å·²ä¼˜åŒ–ï¼šæ·»åŠ  Attention Mask
    phoneme_seq = str(example['phonemes']).split()
    cleaned_phoneme_seq = [p for p in phoneme_seq if p not in ['BLANK', '[MASK]']]
    
    p_ids = [SOS_ID] + \
            [phoneme_to_id.get(p, PAD_ID_PHONEME) for p in cleaned_phoneme_seq] + \
            [EOS_ID]
    
    if len(p_ids) > MAX_P_LEN:
        p_ids = p_ids[:MAX_P_LEN]
        p_ids[-1] = EOS_ID 
    
    attention_mask = [1] * len(p_ids)
    
    padding_len = MAX_P_LEN - len(p_ids)
    if padding_len > 0:
        p_ids.extend([PAD_ID_PHONEME] * padding_len)
        attention_mask.extend([0] * padding_len)

    return {
        'input_ids': p_ids,
        'attention_mask': attention_mask
    }

# ==========================================
# 3. æ•°æ®åŠ è½½
# ==========================================

def load_data(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}. è¯·ç¡®ä¿ train.tsv å’Œ val.tsv å­˜åœ¨ã€‚")
    df = pd.read_csv(file_path, sep='\t').dropna().reset_index(drop=True)
    return Dataset.from_pandas(df)

try:
    train_dataset_raw = load_data('train.tsv')
    val_dataset_raw = load_data('val.tsv')
except FileNotFoundError as e:
    print(f"è‡´å‘½é”™è¯¯: {e}")
    exit()

tokenized_train_p2s = train_dataset_raw.map(preprocess_function, remove_columns=['phonemes', 'sentence'])
tokenized_val_p2s = val_dataset_raw.map(preprocess_function, remove_columns=['phonemes', 'sentence'])
tokenized_train_mpm = train_dataset_raw.map(mpm_preprocess_function, remove_columns=['phonemes', 'sentence'])
tokenized_val_mpm = val_dataset_raw.map(mpm_preprocess_function, remove_columns=['phonemes', 'sentence'])

# ==========================================
# 4. é˜¶æ®µä¸€ï¼šMPM é¢„è®­ç»ƒå‡½æ•°
# ==========================================
def run_mpm_pretraining(train_dataset, val_dataset):
    print("="*50)
    print("ğŸš€ é˜¶æ®µä¸€ï¼šå¼€å§‹ MPM (Masked Phoneme Modeling) é¢„è®­ç»ƒ (ä¼˜åŒ–ç‰ˆ)")
    print("="*50)

    # 4.1 Encoder é…ç½® (åŠ æ·±ç½‘ç»œ)
    encoder_config = BertConfig(
        vocab_size=VOCAB_SIZE_PHONEME, 
        hidden_size=256,         
        num_hidden_layers=8,          # 6 -> 8 å±‚ï¼Œå¢åŠ æ¨ç†æ·±åº¦
        num_attention_heads=4,
        intermediate_size=256 * 4,
        pad_token_id=PAD_ID_PHONEME
    )
    
    mpm_model = BertForMaskedLM(config=encoder_config)
    mpm_model.bert.embeddings.word_embeddings = torch.nn.Embedding(
        VOCAB_SIZE_PHONEME, 256, padding_idx=PAD_ID_PHONEME
    )

    # 4.2 MPM è®­ç»ƒå‚æ•° (å¢åŠ è®­ç»ƒé‡)
    mpm_args = TrainingArguments(
        output_dir=MPM_OUTPUT_DIR,
        num_train_epochs=50,                # 10 -> 50: é¢„è®­ç»ƒéœ€è¦æ›´å¤šè½®æ¬¡æ‰èƒ½æ”¶æ•›
        per_device_train_batch_size=16,     # å°è¯•ç¨å¾®å¢å¤§å•å¡ Batch (å¦‚æœæ˜¾å­˜ä¸å¤Ÿæ”¹å› 8)
        gradient_accumulation_steps=1,      # ç§»é™¤ç´¯ç§¯ï¼Œè®©å‚æ•°æ›´æ–°æ›´é¢‘ç¹
        learning_rate=1e-4,                 # ç¨å¾®é™ä½ä¸€ç‚¹ LRï¼Œé…åˆæ›´å¤šçš„ Epoch
        warmup_ratio=0.1,                   # ä½¿ç”¨æ¯”ä¾‹ Warmup
        weight_decay=0.01,
        logging_steps=50,
        
        eval_strategy="epoch", 
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        save_total_limit=2, 
        
        fp16=torch.cuda.is_available(),
        report_to="none",
    )
    
    mpm_data_collator = PhonemeMaskingDataCollator() 

    mpm_trainer = Trainer(
        model=mpm_model,
        args=mpm_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=mpm_data_collator,
    )

    # 4.5 å¼€å§‹è®­ç»ƒ
    mpm_trainer.train()
    
    print("MPM é¢„è®­ç»ƒå®Œæˆã€‚åŠ è½½æœ€ä½³ Encoder æƒé‡...")
    
    # --- å´©æºƒä¿®å¤é€»è¾‘ ---
    # Trainer å¦‚æœ load_best_model_at_end=Trueï¼Œè®­ç»ƒç»“æŸæ—¶å†…å­˜é‡Œçš„ model å·²ç»æ˜¯æœ€ä½³æ¨¡å‹äº†ã€‚
    # æˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨å» load state dictï¼Œé™¤éä½ æƒ³åŒé‡ä¿é™©ã€‚
    # ä½†ä¸ºäº†è§£å†³ä¹‹å‰çš„æŠ¥é”™ï¼Œæˆ‘ä»¬åŠ ä¸Š safetensors çš„åˆ¤æ–­é€»è¾‘ã€‚
    
    best_ckpt_path = mpm_trainer.state.best_model_checkpoint
    if best_ckpt_path:
        print(f"æœ€ä½³æ£€æŸ¥ç‚¹è·¯å¾„: {best_ckpt_path}")
        # å°è¯•ç›´æ¥ä½¿ç”¨ Trainer å½“å‰çš„æ¨¡å‹ (å®ƒå·²ç»åŠ è½½äº†æœ€ä½³æƒé‡)
        # è¿™æ˜¯ä¸€ä¸ªå°æŠ€å·§ï¼Œé€šå¸¸ Trainer è®­ç»ƒå®Œä¼šè‡ªåŠ¨å›æ»šåˆ°æœ€ä½³æƒé‡
    else:
        print("æœªæ‰¾åˆ°æœ€ä½³æ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨æœ€ç»ˆæ¨¡å‹æƒé‡ã€‚")

    # åªè¦ä¸æŠ¥é”™ï¼Œç›´æ¥è¿”å› mpm_model.bert å³å¯
    # å› ä¸º load_best_model_at_end=True ä¿è¯äº† mpm_model ç°åœ¨å°±æ˜¯æœ€ä½³çŠ¶æ€
    return mpm_model.bert

# ==========================================
# 5. é˜¶æ®µäºŒï¼šP2S å¾®è°ƒè®¾ç½®
# ==========================================

def compute_metrics(eval_pred):
    # å ä½ç¬¦
    predictions, labels = eval_pred
    return {"loss_placeholder": 0.0} 

BATCH_SIZE = 8
STEPS_PER_EPOCH = len(tokenized_train_p2s) // BATCH_SIZE
EVAL_INTERVAL_EPOCHS = 2 
EVAL_STEPS = STEPS_PER_EPOCH * EVAL_INTERVAL_EPOCHS

training_args_p2s = Seq2SeqTrainingArguments(
    output_dir="./p2s_checkpoints_lora_trainable_encoder", 
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=5e-5,
    warmup_steps=100,
    weight_decay=0.01,
    logging_steps=50,
    eval_strategy="steps",
    eval_steps=EVAL_STEPS,
    save_strategy="steps",
    save_steps=EVAL_STEPS,
    save_total_limit=10, 
    predict_with_generate=True,
    generation_max_length=MAX_S_LEN,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    fp16=torch.cuda.is_available(),
    report_to="none"
)

data_collator_p2s = DataCollatorForSeq2Seq(
    tokenizer, 
    model=None, 
    label_pad_token_id=-100,
    pad_to_multiple_of=8 if torch.cuda.is_available() else None
)

# ==========================================
# 6. å¼€å§‹æ‰§è¡Œè®­ç»ƒæµç¨‹
# ==========================================

if __name__ == "__main__":
    
    # --- é˜¶æ®µä¸€ï¼šMPM é¢„è®­ç»ƒ ---
    pretrain_encoder = run_mpm_pretraining(tokenized_train_mpm, tokenized_val_mpm)
    
    # --- é˜¶æ®µäºŒï¼šP2S å¾®è°ƒ ---
    print("\n" + "="*50)
    print("ğŸ“ é˜¶æ®µäºŒï¼šå¼€å§‹ P2S (Phoneme-to-Text) å¾®è°ƒ (LoRA + Encoder å¯è®­ç»ƒ)")
    print("="*50)
    
    encoder_config = pretrain_encoder.config 
    encoder = pretrain_encoder 
    # âš ï¸ å…³é”®ç‚¹ï¼šæˆ‘ä»¬ä¿æŒ encoder çš„ requires_grad=Trueï¼Œä½¿å…¶å¯è®­ç»ƒã€‚

    decoder_config = GPT2Config.from_pretrained(PLM_MODEL_NAME)
    decoder_config.add_cross_attention = True
    decoder_config.is_decoder = True 
    decoder = GPT2LMHeadModel.from_pretrained(PLM_MODEL_NAME, config=decoder_config)
    
    config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)
    model = EncoderDecoderModel(config=config, encoder=encoder, decoder=decoder)

    model.config.decoder_start_token_id = tokenizer.bos_token_id
    model.config.eos_token_id = tokenizer.eos_token_id
    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.vocab_size = model.config.decoder.vocab_size

    model.config.max_length = MAX_S_LEN
    model.config.min_length = 2
    model.config.no_repeat_ngram_size = 3
    model.config.early_stopping = True
    model.config.length_penalty = 1.0
    model.config.num_beams = 4 

    # ==========================================
    # ğŸ”¥ LoRA æ ¸å¿ƒä¿®æ”¹ï¼šé’ˆå¯¹ GPT-2 ç»“æ„ ğŸ”¥
    # ==========================================
    
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        # âš ï¸ ä¿®æ”¹ï¼šGPT-2 ä½¿ç”¨ c_attn å·ç§¯å±‚ï¼Œè€Œä¸æ˜¯ q_proj/v_proj
        target_modules=["c_attn"], 
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.SEQ_TO_SEQ_LM,
    )
    
    peft_decoder = get_peft_model(model.decoder, lora_config)
    model.decoder = peft_decoder
    
    print("\n--- æ¨¡å‹å‚æ•°ä¿¡æ¯ ---")
    peft_decoder.print_trainable_parameters()
    print("Encoder (BERT) å‚æ•°çŠ¶æ€ï¼š")
    
    trainable_params_encoder = sum(p.numel() for p in model.encoder.parameters() if p.requires_grad)
    trainable_params_decoder = sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())

    print(f"  Encoder (å¯è®­ç»ƒ): {trainable_params_encoder:,} (ä¿æŒå¯è®­ç»ƒ)")
    print(f"  Decoder (LoRA å¯è®­ç»ƒ): {trainable_params_decoder:,}")
    print(f"æ€»å¯è®­ç»ƒå‚æ•°: {trainable_params_encoder + trainable_params_decoder:,}")
    print(f"æ€»å‚æ•°: {all_params:,}")
    print("------------------------\n")
    
    trainer_p2s = Seq2SeqTrainer(
        model=model,
        args=training_args_p2s,
        train_dataset=tokenized_train_p2s,
        eval_dataset=tokenized_val_p2s,
        tokenizer=tokenizer,
        data_collator=data_collator_p2s,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]
    )

    print(f"Starting P2S fine-tuning (LoRA + Trainable Encoder) on device: {training_args_p2s.device}")
    trainer_p2s.train()
    
    print("P2S Fine-Tuning finished. Saving final model and LoRA adapter...")
    
    model.save_pretrained("./p2s_final_model_lora_trainable_encoder")
    tokenizer.save_pretrained("./p2s_final_model_lora_trainable_encoder")