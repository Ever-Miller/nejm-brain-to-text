import torch
from transformers import AutoTokenizer, EncoderDecoderModel

# --- 1. é…ç½®å’Œè¯æ±‡è¡¨å®šä¹‰ (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´) ---

# æä¾›çš„éŸ³ç´ åˆ—è¡¨
PHONEMES_LIST = [
    "AA", "AE", "AH", "AO", "AW", "AY", "B", "CH", "D", "DH",
    "EH", "ER", "EY", "F", "G", "HH", "IH", "IY", "JH", "K",
    "L", "M", "N", "NG", "OW", "OY", "P", "R", "S", "SH",
    "T", "TH", "UH", "UW", "V", "W", "Y", "Z", "ZH", "BLANK"
]

# æ·»åŠ ç‰¹æ®Šæ ‡è®°å’Œåˆ†éš”ç¬¦
SPECIAL_TOKENS = ['<pad>', '<sos>', '<eos>', '.', '|']
PHONEME_VOCAB = SPECIAL_TOKENS + PHONEMES_LIST
phoneme_to_id = {p: i for i, p in enumerate(PHONEME_VOCAB)}
PAD_ID_PHONEME = phoneme_to_id['<pad>']
BLANK_ID = phoneme_to_id['BLANK']
VOCAB_SIZE_PHONEME = len(PHONEME_VOCAB)

# æ¨¡å‹å’Œåˆ†è¯å™¨åç§°
PLM_MODEL_NAME = "gpt2"
MODEL_PATH = "./ed/p2s_checkpoints_lora/checkpoint-10000" # <--- æ›¿æ¢ä¸ºæ‚¨çš„æœ€ä½³æ¨¡å‹è·¯å¾„!

# --- 2. æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½ ---

# åŠ è½½ PLM Tokenizerï¼ˆç”¨äºæ–‡æœ¬è§£ç ï¼‰
tokenizer = AutoTokenizer.from_pretrained(PLM_MODEL_NAME)
if not tokenizer.pad_token:
    # GPT-2 é»˜è®¤æ²¡æœ‰ PAD tokenï¼Œå°†å…¶è®¾ç½®ä¸º EOS token (ID 50256)
    tokenizer.pad_token = tokenizer.eos_token 

# ç¡®å®šè®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# åŠ è½½è®­ç»ƒå¥½çš„ Encoder-Decoder æ¨¡å‹
try:
    model = EncoderDecoderModel.from_pretrained(MODEL_PATH)
    model.to(device)
    model.eval()
    print(f"Model loaded successfully from {MODEL_PATH}")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Please check if MODEL_PATH is correct and the model finished training.")
    exit()

# ğŸ’¡ å…³é”®ä¿®å¤æ­¥éª¤ï¼šæ˜¾å¼è®¾ç½® Decoder çš„ç”Ÿæˆé…ç½® (Generation Configuration)
# è¿™è§£å†³äº† GPT-2 ä¸­ PAD, BOS, EOS ID å†²çªçš„é—®é¢˜ï¼Œç¡®ä¿ç”Ÿæˆé€»è¾‘æ­£ç¡®ã€‚
# è¿™ä¸€æ­¥æ˜¯å‰ä¸€æ¬¡ä¿®å¤ä¸­å·²æ·»åŠ çš„ã€‚
model.config.decoder_start_token_id = tokenizer.bos_token_id 
model.config.pad_token_id = tokenizer.pad_token_id           
model.config.eos_token_id = tokenizer.eos_token_id           
print("âœ… Decoder generation config updated.")


# --- 3. æ¨ç†å‡½æ•° (ä¸»è¦ä¿®æ”¹åœ¨ model.generate éƒ¨åˆ†) ---

def phoneme_to_sentence(phoneme_sequence: str, model, tokenizer, device, max_length=50):
    """
    å°†éŸ³ç´ åºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬å¥å­ã€‚
    """
    
    # --- 1. é¢„å¤„ç†éŸ³ç´ åºåˆ— (Encoder Input) ---
    
    phoneme_tokens = phoneme_sequence.split()
    cleaned_phoneme_tokens = [p for p in phoneme_tokens if p != 'BLANK']
    
    # è½¬æ¢ä¸º ID
    p_ids = [phoneme_to_id['<sos>']] + \
            [phoneme_to_id.get(p, PAD_ID_PHONEME) for p in cleaned_phoneme_tokens] + \
            [phoneme_to_id['<eos>']]
    
    # è½¬æ¢ä¸º PyTorch Tensor
    input_ids = torch.tensor([p_ids], dtype=torch.long).to(device)
    
    # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
    attention_mask = (input_ids != PAD_ID_PHONEME).long()

    # --- 2. æ¨¡å‹ç”Ÿæˆ (Generation) ---
    
    with torch.no_grad():
        generated_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            # å¼ºåˆ¶é˜»æ­¢ 2-gram æˆ– 3-gram é‡å¤ï¼Œæœ‰æ•ˆå‡å°‘ "TheThe" æˆ– "a a a" çš„å‡ºç°
            no_repeat_ngram_size=3, 
            # å…¶ä»–å‚æ•°ç°åœ¨åº”åœ¨ model.generation_config ä¸­è®¾ç½®
        )
    
    # --- 3. åå¤„ç†å’Œè§£ç  ---
    
    generated_sentence = tokenizer.decode(
        generated_ids.squeeze().tolist(), 
        skip_special_tokens=True 
    )
    
    return generated_sentence.strip()


# --- 4. ç¤ºä¾‹æ¨ç† ---

example_phonemes = "DH AH | K R UH K AH D | M EY Z | F EY L D | T UW | F UW L | DH AH | M AW S |"

print("\n--- Starting Inference ---")
print(f"Input Phonemes: {example_phonemes}")

# æ‰§è¡Œæ¨ç†
generated_sentence = phoneme_to_sentence(
    example_phonemes, 
    model, 
    tokenizer, 
    device, 
    max_length=50
)

# æ‰“å°ç»“æœ
print(f"\nGenerated Sentence: {generated_sentence}")
print("-" * 30)